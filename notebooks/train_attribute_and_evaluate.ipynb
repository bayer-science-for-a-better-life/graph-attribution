{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T19:50:12.516753Z",
     "start_time": "2020-11-19T19:50:12.497088Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T19:50:15.369290Z",
     "start_time": "2020-11-19T19:50:15.340458Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T19:50:15.593050Z",
     "start_time": "2020-11-19T19:50:15.572136Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "REPO_DIR = '..' if IN_COLAB  else '..'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OD_uoUHdGp5t"
   },
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T20:01:09.511494Z",
     "start_time": "2020-11-19T20:01:09.431182Z"
    },
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "id": "8moWllwb-yZr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rdkit detected? True\n",
      "tensorflow           = 2.4.0\n",
      "sonnet               = 2.0.0\n",
      "graph_attribution    = 1.0.0b\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import itertools\n",
    "import collections\n",
    "import tqdm.auto as tqdm\n",
    "import time\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "import sonnet as snt\n",
    "import graph_nets\n",
    "from graph_nets.graphs import GraphsTuple\n",
    "import graph_attribution as gatt\n",
    "\n",
    "# Ignore tf/graph_nets UserWarning:\n",
    "# Converting sparse IndexedSlices to a dense Tensor of unknown shape\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "\n",
    "for mod in [tf, snt, gatt]:\n",
    "    print(f'{mod.__name__:20s} = {mod.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EAwhhlmhUWHH"
   },
   "source": [
    "## Graph Attribution specific imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T19:58:14.416163Z",
     "start_time": "2020-11-19T19:58:14.337198Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data from: ../data\n"
     ]
    }
   ],
   "source": [
    "from graph_attribution import tasks\n",
    "from graph_attribution import graphnet_models as gnn_models\n",
    "from graph_attribution import graphnet_techniques as techniques\n",
    "from graph_attribution import datasets\n",
    "from graph_attribution import experiments\n",
    "from graph_attribution import templates\n",
    "from graph_attribution import graphs as graph_utils\n",
    "\n",
    "datasets.DATA_DIR = os.path.join(REPO_DIR, 'data')\n",
    "print(f'Reading data from: {datasets.DATA_DIR}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j-1hgin_htxk"
   },
   "source": [
    "# Load Experiment data, a task and attribution techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T19:58:18.979633Z",
     "start_time": "2020-11-19T19:58:18.895459Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "83FuJCHIPy9B"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available tasks: ['benzene', 'logic7', 'logic8', 'logic10', 'crippen', 'bashapes', 'treegrid', 'bacommunity']\n",
      "Available model types: ['gcn', 'gat', 'mpnn', 'graphnet']\n",
      "Available ATT techniques: ['Random', 'GradInput', 'SmoothGrad(GradInput)', 'GradCAM-last', 'GradCAM-all', 'CAM']\n"
     ]
    }
   ],
   "source": [
    "print(f'Available tasks: {[t.name for t in tasks.Task]}')\n",
    "print(f'Available model types: {[m.name for m in gnn_models.BlockType]}')\n",
    "print(f'Available ATT techniques: {list(techniques.get_techniques_dict(None,None).keys())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T19:58:20.566788Z",
     "start_time": "2020-11-19T19:58:19.250832Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "uALixzYcevP4",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes of GraphsTuple's fields:\n",
      "GraphsTuple(nodes=TensorShape([23201, 14]), edges=TensorShape([47078, 5]), receivers=TensorShape([47078]), senders=TensorShape([47078]), globals=TensorShape([901, 1]), n_node=TensorShape([901]), n_edge=TensorShape([901]))\n",
      "Experiment data fields:['x_train', 'x_test', 'y_train', 'y_test', 'att_test', 'x_aug', 'y_aug']\n"
     ]
    }
   ],
   "source": [
    "task_type = 'crippen'\n",
    "block_type = 'gcn'\n",
    "\n",
    "task_dir = datasets.get_task_dir(task_type)\n",
    "exp, task, methods = experiments.get_experiment_setup(task_type, block_type)\n",
    "task_act, task_loss = task.get_nn_activation_fn(), task.get_nn_loss_fn()\n",
    "graph_utils.print_graphs_tuple(exp.x_train)\n",
    "print(f'Experiment data fields:{list(exp.__dict__.keys())}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oa5pg38yukMU"
   },
   "source": [
    "# Create a GNN model\n",
    "\n",
    "## Define hparams of the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T19:58:20.635213Z",
     "start_time": "2020-11-19T19:58:20.568997Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "zYKgsgksQ6T6"
   },
   "outputs": [],
   "source": [
    "hp = gatt.hparams.get_hparams({'block_type':block_type, 'task_type':task_type})\n",
    "hp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0eHoQUuCSR49"
   },
   "source": [
    "## Instantiate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T19:58:23.774392Z",
     "start_time": "2020-11-19T19:58:22.445118Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "xFD0Z2psUslb"
   },
   "outputs": [],
   "source": [
    "model = experiments.GNN(node_size = hp.node_size,\n",
    "               edge_size = hp.edge_size,\n",
    "               global_size = hp.global_size,\n",
    "               y_output_size = task.n_outputs,\n",
    "               block_type = gnn_models.BlockType(hp.block_type),\n",
    "               activation = task_act,\n",
    "               target_type = task.target_type,\n",
    "               n_layers = hp.n_layers)\n",
    "model(exp.x_train)\n",
    "gnn_models.print_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OyH67z9gaCZ_"
   },
   "source": [
    "# Train model on task\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "baX60p7WWXL-"
   },
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Tuple\n",
    "\n",
    "def get_batch_indices(n: int, batch_size: int) -> np.ndarray:\n",
    "    \"\"\"Gets shuffled constant size batch indices to train a model.\"\"\"\n",
    "    n_batches = n // batch_size\n",
    "    indices = tf.random.shuffle(tf.range(n))\n",
    "    indices = indices[:n_batches * batch_size]\n",
    "    indices = tf.reshape(indices, (n_batches, batch_size))\n",
    "    return indices\n",
    "\n",
    "def make_tf_opt_epoch_fn(\n",
    "        inputs: GraphsTuple, target: np.ndarray, batch_size: int, model: snt.Module,\n",
    "        optimizer: snt.Optimizer, loss_fn: templates.LossFunction,\n",
    "        l2_reg: float = 0.0, orth_lambda: float = 0.0) -> Callable[[tf.Tensor, tf.Tensor], tf.Tensor]:\n",
    "    \"\"\"Make a tf.function of (inputs, target) for optimization.\n",
    "\n",
    "    This function is useful for basic inference training of GNN models. Uses all\n",
    "    variables to create a a function that has a tf.function optimized input\n",
    "    signature. Function uses pure tf.functions to build batches and aggregate\n",
    "    losses. The result is a heavily optimized function that is at least 2x\n",
    "    faster than a basic tf.function with experimental_relax_shapes=True.\n",
    "\n",
    "    Args:\n",
    "      inputs: graphs used for training.\n",
    "      target: values to predict for training.\n",
    "      batch_size: batch size.\n",
    "      model: a GNN model.\n",
    "      optimizer: optimizer, probably Adam or SGD.\n",
    "      loss_fn: a loss function to optimize.\n",
    "      l2_reg: l2 regularization weight.\n",
    "\n",
    "    Returns:\n",
    "      optimize_one_epoch(intpus, target), a tf.function optimized\n",
    "      callable.\n",
    "\n",
    "    \"\"\"\n",
    "    # Explicit input signature is faster than experimental relax shapes.\n",
    "    input_signature = [\n",
    "        graph_nets.utils_tf.specs_from_graphs_tuple(inputs),\n",
    "        tf.TensorSpec.from_tensor(tf.convert_to_tensor(target))\n",
    "    ]\n",
    "    n = graph_utils.get_num_graphs(inputs)\n",
    "    n_batches = tf.cast(n // batch_size, tf.float32)\n",
    "\n",
    "    if l2_reg > 0.0:\n",
    "        regularizer = snt.regularizers.L2(l2_reg)\n",
    "        linear_variables = gnn_models.get_linear_variables(model)\n",
    "    if batch_size == 1 or n == 1:\n",
    "        def optimize_one_epoch(inputs, target):\n",
    "            \"\"\"One epoch single-batch optimization.\"\"\"\n",
    "            with tf.GradientTape() as tape:\n",
    "                loss = loss_fn(target, model(inputs))\n",
    "                if l2_reg > 0.0:\n",
    "                    loss += regularizer(linear_variables)\n",
    "\n",
    "            grads = tape.gradient(loss, model.trainable_variables)\n",
    "            optimizer.apply(grads, model.trainable_variables)\n",
    "            return loss\n",
    "    else:\n",
    "        def optimize_one_epoch(inputs, target):\n",
    "            \"\"\"One epoch optimization.\"\"\"\n",
    "            loss = tf.constant(0.0, tf.float32)\n",
    "            for batch in get_batch_indices(n, batch_size):\n",
    "                x_batch = graph_utils.get_graphs_tf(inputs, batch)\n",
    "                y_batch = tf.gather(target, batch)\n",
    "                with tf.GradientTape() as tape:\n",
    "                    output, representations = model(x_batch)\n",
    "                    \n",
    "                    # custom loss code here\n",
    "                    t = tf.cumsum(representations.n_node, exclusive=True)\n",
    "                    orth_loss = tf.constant(0.0, tf.float32)\n",
    "                    for i in range(1, len(t)):\n",
    "                        w = representations.nodes[t[i-1]:t[i]]\n",
    "                        ws = w @ tf.transpose(w)\n",
    "                        ws -= tf.linalg.eye(representations.n_node[i - 1])\n",
    "                        orth_loss += tf.norm(ws, ord=2, axis=[-2, -1])\n",
    "                            \n",
    "                    batch_loss = loss_fn(y_batch, output)\n",
    "                    if l2_reg > 0.0:\n",
    "                        batch_loss += regularizer(linear_variables)\n",
    "                    if orth_lambda > 0.0:\n",
    "                        batch_loss += orth_lambda / 2 * orth_loss / batch_size\n",
    "\n",
    "                grads = tape.gradient(batch_loss, model.trainable_variables)\n",
    "                optimizer.apply(grads, model.trainable_variables)\n",
    "                loss += batch_loss\n",
    "            return loss / n_batches\n",
    "\n",
    "    return tf.function(optimize_one_epoch, input_signature=input_signature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T20:06:19.165556Z",
     "start_time": "2020-11-19T20:01:50.383747Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "BOxaREIYt1yp",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "optimizer = snt.optimizers.Adam(hp.learning_rate)\n",
    "\n",
    "\n",
    "\n",
    "opt_one_epoch = make_tf_opt_epoch_fn(exp.x_train, exp.y_train, hp.batch_size, model,\n",
    "                                      optimizer, task_loss, l2_reg=0.0, orth_lambda=0.000)\n",
    "\n",
    "pbar = tqdm.tqdm(range(hp.epochs))\n",
    "losses = collections.defaultdict(list)\n",
    "start_time = time.time()\n",
    "for _ in pbar:\n",
    "    train_loss = opt_one_epoch(exp.x_train, exp.y_train).numpy()\n",
    "    losses['train'].append(train_loss)\n",
    "    r = model(exp.x_test);\n",
    "    losses['test'].append(task_loss(exp.y_test, model(exp.x_test)[0]).numpy())\n",
    "    pbar.set_postfix({key: values[-1] for key, values in losses.items()})\n",
    "\n",
    "losses = {key: np.array(values) for key, values in losses.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AhopNuNnU4ZP"
   },
   "source": [
    "## Plot losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T20:06:21.401827Z",
     "start_time": "2020-11-19T20:06:21.123503Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "x-l_IsAKOj__",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for key, values in losses.items():\n",
    "  plt.plot(values, label=key)\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epochs')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T20:09:10.663044Z",
     "start_time": "2020-11-19T20:06:30.124486Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "Ytyo5Om_4evs",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "for method in tqdm.tqdm(methods.values(), total=len(methods)):\n",
    "  results.append(experiments.generate_result(model, method, task, exp.x_test, exp.y_test, exp.att_test))\n",
    "pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cE8ivqFFmlBk"
   },
   "source": [
    "# Evaluate predictions and attributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(task_type, block_type, orth=0.00):\n",
    "    task_dir = datasets.get_task_dir(task_type)\n",
    "    exp, task, methods = experiments.get_experiment_setup(task_type, block_type)\n",
    "    task_act, task_loss = task.get_nn_activation_fn(), task.get_nn_loss_fn()\n",
    "    graph_utils.print_graphs_tuple(exp.x_train)\n",
    "    \n",
    "    hp = gatt.hparams.get_hparams({'block_type':block_type, 'task_type':task_type})\n",
    "    \n",
    "    model = experiments.GNN(node_size = hp.node_size,\n",
    "               edge_size = hp.edge_size,\n",
    "               global_size = hp.global_size,\n",
    "               y_output_size = task.n_outputs,\n",
    "               block_type = gnn_models.BlockType(hp.block_type),\n",
    "               activation = task_act,\n",
    "               target_type = task.target_type,\n",
    "               n_layers = hp.n_layers)\n",
    "    model(exp.x_train)\n",
    "\n",
    "    optimizer = snt.optimizers.Adam(hp.learning_rate)\n",
    "\n",
    "\n",
    "\n",
    "    opt_one_epoch = make_tf_opt_epoch_fn(exp.x_train, exp.y_train, hp.batch_size, model,\n",
    "                                          optimizer, task_loss, l2_reg=0.0, orth_lambda=0.000)\n",
    "\n",
    "    pbar = tqdm.tqdm(range(hp.epochs))\n",
    "    losses = collections.defaultdict(list)\n",
    "    start_time = time.time()\n",
    "    for _ in pbar:\n",
    "        train_loss = opt_one_epoch(exp.x_train, exp.y_train).numpy()\n",
    "        losses['train'].append(train_loss)\n",
    "        r = model(exp.x_test);\n",
    "        losses['test'].append(task_loss(exp.y_test, model(exp.x_test)[0]).numpy())\n",
    "        pbar.set_postfix({key: values[-1] for key, values in losses.items()})\n",
    "\n",
    "    losses = {key: np.array(values) for key, values in losses.items()}\n",
    "    \n",
    "    results = []\n",
    "    for method in tqdm.tqdm(methods.values(), total=len(methods)):\n",
    "        results.append(experiments.generate_result(model, method, task, exp.x_test, exp.y_test, exp.att_test))\n",
    "        \n",
    "    return pd.DataFrame(results), losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes of GraphsTuple's fields:\n",
      "GraphsTuple(nodes=TensorShape([205826, 14]), edges=TensorShape([436434, 5]), receivers=TensorShape([436434]), senders=TensorShape([436434]), globals=TensorShape([10000, 1]), n_node=TensorShape([10000]), n_edge=TensorShape([10000]))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4726b5708adc4ba092d41c9f95c405e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=300.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_trials = 5\n",
    "block = ['gcn', 'gat', 'mpnn', 'graphnet']\n",
    "task = ['benzene', 'logic10', 'crippen']\n",
    "orth = [0.000, 0.001]\n",
    "run = range(n_trials)\n",
    "\n",
    "results = {}\n",
    "losses = {}\n",
    "\n",
    "for block, task, orth, run in product(block, task, orth, run):\n",
    "    try:\n",
    "        result, loss = train_and_evaluate(task, block, orth)\n",
    "        results[(block, task, orth, run)] = result\n",
    "        loss[(block, task, orth, run)] = loss\n",
    "    except Exception as e:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize attributions"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "last_runtime": {
    "build_target": "",
    "kind": "private"
   },
   "name": "train_evaluate.ipynb",
   "provenance": [
    {
     "file_id": "/piper/depot/google3/experimental/graph_attribution/graphnets_demo.ipynb",
     "timestamp": 1589329304332
    },
    {
     "file_id": "/piper/depot/google3/experimental/graph_attribution/graphnets_demo.ipynb",
     "timestamp": 1580393977236
    },
    {
     "file_id": "1BfVceP2yhGgdtloEZNy_lR1NP7pjWhez",
     "timestamp": 1579666920169
    },
    {
     "file_id": "1QyHsdvTHd2a4SPXCGtEsd3rL7pT2S6vO",
     "timestamp": 1578435769855
    },
    {
     "file_id": "1kzXpb_wct-EnLPOAvTvRXGCGOiLK7SkP",
     "timestamp": 1578350485474
    },
    {
     "file_id": "/piper/depot/google3/learning/deepmind/tensorflow/graph_nets/tf2/demos/sort.ipynb",
     "timestamp": 1577987457433
    },
    {
     "file_id": "13QaQPnwO8Iq5YjoSraE2Gt_jz3f9e8pc",
     "timestamp": 1571434083446
    },
    {
     "file_id": "/piper/depot/google3/third_party/py/graph_nets/demos/sort.ipynb",
     "timestamp": 1570733816658
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
